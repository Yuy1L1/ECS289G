{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21251,"status":"ok","timestamp":1668967216434,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"lwTcxqGuluxT","outputId":"9d8de494-61f7-4b69-89e6-1f473379bae0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/mydrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/mydrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1668967216435,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"ZmhIZ3pjp8cq"},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1668967216435,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"nX1t6WgRp9_O"},"outputs":[],"source":["os.chdir(\"/content/mydrive/MyDrive/DeepSpeechv\")"]},{"cell_type":"markdown","metadata":{"id":"Q-AM8GkEbbhB"},"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1668878987626,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"oiX-neudqN2x"},"outputs":[],"source":["#pwd"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1668878987626,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"UeWlqKd3qQQ-"},"outputs":[],"source":["#!tar -xvf DeepSpeech-0.9.3.tar.gz"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1668878987626,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"tb5F8Taitdbx"},"outputs":[],"source":["#!tar -xvf deepspeech-0.9.3-checkpoint.tar.gz"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1515,"status":"ok","timestamp":1668967217941,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"f4iIbvUszcJR"},"outputs":[],"source":["os.chdir('/content/mydrive/MyDrive/DeepSpeechv/DeepSpeech')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":29241,"status":"ok","timestamp":1668967247175,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"dWWyP3mIz2rC","outputId":"756fb3c7-db41-476d-ed6e-bcb34b0f2047"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting absl-py==0.9.0\n","  Downloading absl-py-0.9.0.tar.gz (104 kB)\n","\u001b[K     |████████████████████████████████| 104 kB 4.4 MB/s \n","\u001b[?25hCollecting attrdict==2.0.1\n","  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n","Collecting deepspeech\n","  Downloading deepspeech-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (9.2 MB)\n","\u001b[K     |████████████████████████████████| 9.2 MB 23.8 MB/s \n","\u001b[?25hCollecting numpy==1.16.0\n","  Downloading numpy-1.16.0-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n","\u001b[K     |████████████████████████████████| 17.3 MB 24.7 MB/s \n","\u001b[?25hCollecting progressbar2==3.47.0\n","  Downloading progressbar2-3.47.0-py2.py3-none-any.whl (24 kB)\n","Collecting python-utils==2.3.0\n","  Downloading python_utils-2.3.0-py2.py3-none-any.whl (12 kB)\n","Collecting six==1.13.0\n","  Downloading six-1.13.0-py2.py3-none-any.whl (10 kB)\n","Collecting pandas==0.25.3\n","  Downloading pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4 MB)\n","\u001b[K     |████████████████████████████████| 10.4 MB 36.3 MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->-r requirements_eval_tflite.txt (line 8)) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->-r requirements_eval_tflite.txt (line 8)) (2.8.2)\n","Building wheels for collected packages: absl-py\n","  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121940 sha256=eb51caf7f9e0e9c84759f8ee8326eae460fcda9820562703288084695a4208a9\n","  Stored in directory: /root/.cache/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\n","Successfully built absl-py\n","Installing collected packages: six, python-utils, numpy, progressbar2, pandas, deepspeech, attrdict, absl-py\n","  Attempting uninstall: six\n","    Found existing installation: six 1.15.0\n","    Uninstalling six-1.15.0:\n","      Successfully uninstalled six-1.15.0\n","  Attempting uninstall: python-utils\n","    Found existing installation: python-utils 3.4.5\n","    Uninstalling python-utils-3.4.5:\n","      Successfully uninstalled python-utils-3.4.5\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: progressbar2\n","    Found existing installation: progressbar2 3.38.0\n","    Uninstalling progressbar2-3.38.0:\n","      Successfully uninstalled progressbar2-3.38.0\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.3.5\n","    Uninstalling pandas-1.3.5:\n","      Successfully uninstalled pandas-1.3.5\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 1.3.0\n","    Uninstalling absl-py-1.3.0:\n","      Successfully uninstalled absl-py-1.3.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\n","xarray 0.20.2 requires numpy>=1.18, but you have numpy 1.16.0 which is incompatible.\n","xarray 0.20.2 requires pandas>=1.1, but you have pandas 0.25.3 which is incompatible.\n","xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.16.0 which is incompatible.\n","tensorflow 2.9.2 requires absl-py>=1.0.0, but you have absl-py 0.9.0 which is incompatible.\n","tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.16.0 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.16.0 which is incompatible.\n","scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.16.0 which is incompatible.\n","scikit-image 0.18.3 requires numpy>=1.16.5, but you have numpy 1.16.0 which is incompatible.\n","resampy 0.4.2 requires numpy>=1.17, but you have numpy 1.16.0 which is incompatible.\n","pywavelets 1.3.0 requires numpy>=1.17.3, but you have numpy 1.16.0 which is incompatible.\n","pyerfa 2.0.0.1 requires numpy>=1.17, but you have numpy 1.16.0 which is incompatible.\n","pyarrow 6.0.1 requires numpy>=1.16.6, but you have numpy 1.16.0 which is incompatible.\n","prophet 1.1.1 requires pandas>=1.0.4, but you have pandas 0.25.3 which is incompatible.\n","plotnine 0.8.0 requires numpy>=1.19.0, but you have numpy 1.16.0 which is incompatible.\n","plotnine 0.8.0 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n","pandas-gbq 0.17.9 requires numpy>=1.16.6, but you have numpy 1.16.0 which is incompatible.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.16.0 which is incompatible.\n","mizani 0.7.3 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n","kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.0 which is incompatible.\n","jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.16.0 which is incompatible.\n","jax 0.3.23 requires numpy>=1.20, but you have numpy 1.16.0 which is incompatible.\n","gym 0.25.2 requires numpy>=1.18.0, but you have numpy 1.16.0 which is incompatible.\n","google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n","db-dtypes 1.0.4 requires numpy<2.0dev,>=1.16.6, but you have numpy 1.16.0 which is incompatible.\n","cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.16.0 which is incompatible.\n","astropy 4.3.1 requires numpy>=1.17, but you have numpy 1.16.0 which is incompatible.\n","aesara 2.7.9 requires numpy>=1.17.0, but you have numpy 1.16.0 which is incompatible.\n","aeppl 0.0.33 requires numpy>=1.18.1, but you have numpy 1.16.0 which is incompatible.\u001b[0m\n","Successfully installed absl-py-0.9.0 attrdict-2.0.1 deepspeech-0.9.3 numpy-1.16.0 pandas-0.25.3 progressbar2-3.47.0 python-utils-2.3.0 six-1.13.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","six"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip3 install -r requirements_eval_tflite.txt"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"executionInfo":{"elapsed":7295,"status":"ok","timestamp":1668967254464,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"zaBKjoyl1110","outputId":"7bdda0ce-5a44-4ec8-d336-9319730b16ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from -r requirements_tests.txt (line 1)) (0.9.0)\n","Collecting argparse\n","  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n","Collecting semver\n","  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->-r requirements_tests.txt (line 1)) (1.13.0)\n","Installing collected packages: semver, argparse\n","Successfully installed argparse-1.4.0 semver-2.13.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["argparse"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip3 install -r requirements_tests.txt"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11998,"status":"ok","timestamp":1668967266454,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"OSLeES8r2KfW","outputId":"df12c692-022c-4111-a497-397673f360dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting webrtcvad\n","  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\n","\u001b[K     |████████████████████████████████| 66 kB 2.4 MB/s \n","\u001b[?25hBuilding wheels for collected packages: webrtcvad\n","  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp37-cp37m-linux_x86_64.whl size=72246 sha256=5dd85fff11b9751ff4e3764df2ca36c6c1a2792baad21d07e1032fa4c6e1815a\n","  Stored in directory: /root/.cache/pip/wheels/11/f9/67/a3158d131f57e1c0a7d8d966a707d4a2fb27567a4fe47723ad\n","Successfully built webrtcvad\n","Installing collected packages: webrtcvad\n","Successfully installed webrtcvad-2.0.10\n"]}],"source":["!pip3 install -r requirements_transcribe.txt"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1668879030411,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"-Yb3HBID4Ysm"},"outputs":[],"source":["#!tar xvf audio-0.9.3.tar.gz"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":82575,"status":"ok","timestamp":1668967349018,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"RJ48dOUYO0k9","outputId":"0bf9de27-cb29-4ad9-b344-b99ee74c688c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pip==20.2.2\n","  Downloading pip-20.2.2-py2.py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 4.4 MB/s \n","\u001b[?25hCollecting wheel==0.34.2\n","  Downloading wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n","Collecting setuptools==49.6.0\n","  Downloading setuptools-49.6.0-py3-none-any.whl (803 kB)\n","\u001b[K     |████████████████████████████████| 803 kB 39.5 MB/s \n","\u001b[?25hInstalling collected packages: wheel, setuptools, pip\n","  Attempting uninstall: wheel\n","    Found existing installation: wheel 0.38.3\n","    Uninstalling wheel-0.38.3:\n","      Successfully uninstalled wheel-0.38.3\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\n","tensorflow 2.9.2 requires absl-py>=1.0.0, but you have absl-py 0.9.0 which is incompatible.\n","tensorflow 2.9.2 requires numpy>=1.20, but you have numpy 1.16.0 which is incompatible.\n","prophet 1.1.1 requires pandas>=1.0.4, but you have pandas 0.25.3 which is incompatible.\n","prophet 1.1.1 requires wheel>=0.37.0, but you have wheel 0.34.2 which is incompatible.\n","pip-tools 6.2.0 requires pip>=20.3, but you have pip 20.2.2 which is incompatible.\n","pandas-gbq 0.17.9 requires numpy>=1.16.6, but you have numpy 1.16.0 which is incompatible.\n","numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.16.0 which is incompatible.\n","kapre 0.3.7 requires numpy>=1.18.5, but you have numpy 1.16.0 which is incompatible.\n","google-colab 1.0.0 requires pandas>=1.1.0, but you have pandas 0.25.3 which is incompatible.\n","aesara 2.7.9 requires numpy>=1.17.0, but you have numpy 1.16.0 which is incompatible.\n","aeppl 0.0.33 requires numpy>=1.18.1, but you have numpy 1.16.0 which is incompatible.\u001b[0m\n","Successfully installed pip-20.2.2 setuptools-49.6.0 wheel-0.34.2\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pkg_resources"]}}},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (1.16.0)\n","Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (3.47.0)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (1.13.0)\n","Collecting pyxdg\n","  Downloading pyxdg-0.28-py2.py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 2.5 MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: attrdict in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (2.0.1)\n","Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (0.9.0)\n","Requirement already satisfied, skipping upgrade: semver in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (2.13.0)\n","Collecting opuslib==2.0.0\n","  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n","Collecting optuna\n","  Downloading optuna-3.0.3-py3-none-any.whl (348 kB)\n","\u001b[K     |████████████████████████████████| 348 kB 7.5 MB/s \n","\u001b[?25hCollecting sox\n","  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n","Requirement already satisfied, skipping upgrade: bs4 in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (0.0.1)\n","Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (0.25.3)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (2.23.0)\n","Collecting numba==0.47.0\n","  Downloading numba-0.47.0-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)\n","\u001b[K     |████████████████████████████████| 3.7 MB 34.8 MB/s \n","\u001b[?25hCollecting llvmlite==0.31.0\n","  Downloading llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n","\u001b[K     |████████████████████████████████| 20.2 MB 1.4 MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: librosa in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (0.8.1)\n","Requirement already satisfied, skipping upgrade: soundfile in /usr/local/lib/python3.7/dist-packages (from deepspeech-training==0.9.3) (0.11.0)\n","Collecting ds_ctcdecoder==0.9.3\n","  Downloading ds_ctcdecoder-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 33.9 MB/s \n","\u001b[?25hCollecting tensorflow==1.15.4\n","  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n","\u001b[K     |████████████████████████████████| 110.5 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from progressbar2->deepspeech-training==0.9.3) (2.3.0)\n","Collecting cmaes>=0.8.2\n","  Downloading cmaes-0.9.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (4.64.1)\n","Requirement already satisfied, skipping upgrade: importlib-metadata<5.0.0 in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (4.13.0)\n","Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (6.0)\n","Requirement already satisfied, skipping upgrade: sqlalchemy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (1.4.43)\n","Collecting colorlog\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (21.3)\n","Requirement already satisfied, skipping upgrade: scipy<1.9.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from optuna->deepspeech-training==0.9.3) (1.7.3)\n","Collecting alembic>=1.5.0\n","  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 56.0 MB/s \n","\u001b[?25hCollecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 9.7 MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->deepspeech-training==0.9.3) (4.6.3)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas->deepspeech-training==0.9.3) (2.8.2)\n","Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->deepspeech-training==0.9.3) (2022.6)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->deepspeech-training==0.9.3) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->deepspeech-training==0.9.3) (1.24.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->deepspeech-training==0.9.3) (2.10)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->deepspeech-training==0.9.3) (2022.9.24)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.47.0->deepspeech-training==0.9.3) (49.6.0)\n","Requirement already satisfied, skipping upgrade: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (3.0.0)\n","Requirement already satisfied, skipping upgrade: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (1.0.2)\n","Requirement already satisfied, skipping upgrade: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (4.4.2)\n","Requirement already satisfied, skipping upgrade: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (0.4.2)\n","Requirement already satisfied, skipping upgrade: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (1.2.0)\n","Requirement already satisfied, skipping upgrade: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa->deepspeech-training==0.9.3) (1.6.0)\n","Requirement already satisfied, skipping upgrade: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile->deepspeech-training==0.9.3) (1.15.1)\n","Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (1.50.0)\n","Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (3.3.0)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (2.1.0)\n","Collecting tensorflow-estimator==1.15.1\n","  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n","\u001b[K     |████████████████████████████████| 503 kB 47.6 MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (0.2.0)\n","Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (0.34.2)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (1.1.2)\n","Collecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (0.8.1)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (3.19.6)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 30.9 MB/s \n","\u001b[?25hCollecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.4->deepspeech-training==0.9.3) (1.14.1)\n","Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna->deepspeech-training==0.9.3) (4.1.1)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5.0.0->optuna->deepspeech-training==0.9.3) (3.10.0)\n","Requirement already satisfied, skipping upgrade: greenlet!=0.4.17; python_version >= \"3\" and (platform_machine == \"aarch64\" or (platform_machine == \"ppc64le\" or (platform_machine == \"x86_64\" or (platform_machine == \"amd64\" or (platform_machine == \"AMD64\" or (platform_machine == \"win32\" or platform_machine == \"WIN32\")))))) in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.0->optuna->deepspeech-training==0.9.3) (2.0.1)\n","Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna->deepspeech-training==0.9.3) (3.0.9)\n","Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from alembic>=1.5.0->optuna->deepspeech-training==0.9.3) (5.10.0)\n","Collecting Mako\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 7.5 MB/s \n","\u001b[?25hCollecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.2-py3-none-any.whl (147 kB)\n","\u001b[K     |████████████████████████████████| 147 kB 59.6 MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna->deepspeech-training==0.9.3) (3.5.0)\n","Collecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Collecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.2-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.9 MB/s \n","\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.11.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 47.4 MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->deepspeech-training==0.9.3) (3.1.0)\n","Requirement already satisfied, skipping upgrade: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->deepspeech-training==0.9.3) (1.4.4)\n","Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile->deepspeech-training==0.9.3) (2.21)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4->deepspeech-training==0.9.3) (3.1.0)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->deepspeech-training==0.9.3) (1.0.1)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4->deepspeech-training==0.9.3) (3.4.1)\n","Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=1.5.0->optuna->deepspeech-training==0.9.3) (2.0.1)\n","Requirement already satisfied, skipping upgrade: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->deepspeech-training==0.9.3) (0.2.5)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied, skipping upgrade: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna->deepspeech-training==0.9.3) (22.1.0)\n","Requirement already satisfied, skipping upgrade: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.4->deepspeech-training==0.9.3) (1.5.2)\n","Building wheels for collected packages: opuslib, gast, pyperclip\n","  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=4572a867de2f86dde73fee0a08387533610f37da90c8553669b3930b36f01955\n","  Stored in directory: /root/.cache/pip/wheels/e5/ba/d4/0e81231a9797fbb262ae3a54fd761fab850db7f32d94a3283a\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=caf549d9227ed10873ccec0f71e52f41789587828f01614891cc3b60b929800a\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11107 sha256=ee02c98f2d78ed47d43ba78b004ab6500c31422eb8bb252feedd5bb186d11c55\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built opuslib gast pyperclip\n","Installing collected packages: pyxdg, opuslib, cmaes, colorlog, Mako, alembic, pyperclip, cmd2, autopage, pbr, stevedore, cliff, optuna, sox, llvmlite, numba, ds-ctcdecoder, tensorflow-estimator, keras-applications, tensorboard, gast, tensorflow, deepspeech-training\n","  Attempting uninstall: llvmlite\n","    Found existing installation: llvmlite 0.39.1\n","    Uninstalling llvmlite-0.39.1:\n","      Successfully uninstalled llvmlite-0.39.1\n","  Attempting uninstall: numba\n","    Found existing installation: numba 0.56.4\n","    Uninstalling numba-0.56.4:\n","      Successfully uninstalled numba-0.56.4\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.9.0\n","    Uninstalling tensorflow-estimator-2.9.0:\n","      Successfully uninstalled tensorflow-estimator-2.9.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.9.1\n","    Uninstalling tensorboard-2.9.1:\n","      Successfully uninstalled tensorboard-2.9.1\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.9.2\n","    Uninstalling tensorflow-2.9.2:\n","      Successfully uninstalled tensorflow-2.9.2\n","  Running setup.py develop for deepspeech-training\n","\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n","\n","We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n","\n","tensorflow-probability 0.17.0 requires gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\n","resampy 0.4.2 requires numba>=0.53, but you'll have numba 0.47.0 which is incompatible.\n","resampy 0.4.2 requires numpy>=1.17, but you'll have numpy 1.16.0 which is incompatible.\n","kapre 0.3.7 requires numpy>=1.18.5, but you'll have numpy 1.16.0 which is incompatible.\n","kapre 0.3.7 requires tensorflow>=2.0.0, but you'll have tensorflow 1.15.4 which is incompatible.\u001b[0m\n","Successfully installed Mako-1.2.4 alembic-1.8.1 autopage-0.5.1 cliff-3.10.1 cmaes-0.9.0 cmd2-2.4.2 colorlog-6.7.0 deepspeech-training ds-ctcdecoder-0.9.3 gast-0.2.2 keras-applications-1.0.8 llvmlite-0.31.0 numba-0.47.0 optuna-3.0.3 opuslib-2.0.0 pbr-5.11.0 pyperclip-1.8.2 pyxdg-0.28 sox-1.4.1 stevedore-3.5.2 tensorboard-1.15.0 tensorflow-1.15.4 tensorflow-estimator-1.15.1\n"]}],"source":["!pip3 install --upgrade pip==20.2.2 wheel==0.34.2 setuptools==49.6.0\n","!pip3 install --upgrade -e ."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1668879101712,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"ijmxpWUR4zCL"},"outputs":[],"source":["# Testing the original deepspeech model \n","#!deepspeech --model deepspeech-0.9.3-models.pbmm --scorer deepspeech-0.9.3-models.scorer --audio audio/BB9.WAV"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1668879101712,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"J622quvxFYMK"},"outputs":[],"source":["#!pip3 uninstall tensorflow\n","#!pip3 install 'tensorflow-gpu==1.15.0'"]},{"cell_type":"markdown","metadata":{"id":"bBSTjfVTTtWM"},"source":[]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1668967349019,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"UNqYffRqMeOr"},"outputs":[],"source":["os.chdir('/content/mydrive/MyDrive/DeepSpeechv/DeepSpeech')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1112,"status":"ok","timestamp":1668967350125,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"owcYCCNFPaJl","outputId":"a8ed29ae-13e5-4f95-e2b8-dfdce8b09e33"},"outputs":[{"name":"stdout","output_type":"stream","text":["make: 'Dockerfile.train' is up to date.\n"]}],"source":["!make Dockerfile.train"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1668879102600,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"p70pXjCivrbz"},"outputs":[],"source":["# Process data and divide it into 3 csv files for train, test and validation\n","#!python3 dataNemorous.py\n","#!python3 dataTorgo.py"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4403,"status":"ok","timestamp":1668994559182,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"RXZ5JQOUA90k","outputId":"ddfb77f4-56ad-4e85-d2a9-fa0b70dea620"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/scipy/__init__.py:149: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.16.0\n","  UserWarning)\n","Traceback (most recent call last):\n","  File \"DeepSpeech.py\", line 12, in <module>\n","    ds_train.run_script()\n","  File \"/content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/training/deepspeech_training/train.py\", line 982, in run_script\n","    absl.app.run(main)\n","  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 299, in run\n","    _run_main(main, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 250, in _run_main\n","    sys.exit(main(argv))\n","  File \"/content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/training/deepspeech_training/train.py\", line 954, in main\n","    train()\n","  File \"/content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/training/deepspeech_training/train.py\", line 427, in train\n","    buffering=FLAGS.read_buffer)\n","  File \"/content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/training/deepspeech_training/util/feeding.py\", line 141, in create_dataset\n","    .map(process_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE))\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 1913, in map\n","    self, map_func, num_parallel_calls, preserve_cardinality=False))\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 3472, in __init__\n","    use_legacy_function=use_legacy_function)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 2713, in __init__\n","    self._function = wrapper_fn._get_concrete_function_internal()\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 1853, in _get_concrete_function_internal\n","    *args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 1847, in _get_concrete_function_internal_garbage_collected\n","    graph_function, _, _ = self._maybe_define_function(args, kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2147, in _maybe_define_function\n","    graph_function = self._create_graph_function(args, kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/eager/function.py\", line 2038, in _create_graph_function\n","    capture_by_value=self._capture_by_value),\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py\", line 915, in func_graph_from_py_func\n","    func_outputs = python_func(*func_args, **func_kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 2707, in wrapper_fn\n","    ret = _wrapper_helper(*args)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 2652, in _wrapper_helper\n","    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 234, in wrapper\n","    return converted_call(f, options, args, kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/api.py\", line 506, in converted_call\n","    converted_f = conversion.convert(target_entity, program_ctx)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 322, in convert\n","    free_nonglobal_var_names)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 240, in _convert_with_cache\n","    entity, program_ctx)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 469, in convert_entity_to_ast\n","    nodes, name, entity_info = convert_func_to_ast(o, program_ctx)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/impl/conversion.py\", line 630, in convert_func_to_ast\n","    node, source = parser.parse_entity(f, future_features=future_features)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/pyct/parser.py\", line 53, in parse_entity\n","    original_source = inspect_utils.getimmediatesource(entity)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/pyct/inspect_utils.py\", line 126, in getimmediatesource\n","    _fix_linecache_record(obj)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/autograph/pyct/inspect_utils.py\", line 118, in _fix_linecache_record\n","    if hasattr(m, '__file__') and m.__file__ == obj_file:\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 50, in __getattr__\n","    module = self._load()\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\", line 44, in _load\n","    module = _importlib.import_module(self.__name__)\n","  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n","    return _bootstrap._gcd_import(name[level:], package, level)\n","  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n","  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n","  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n","  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/__init__.py\", line 47, in <module>\n","    from tensorflow.contrib import distributions\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/distributions/__init__.py\", line 29, in <module>\n","    from tensorflow.contrib.distributions.python.ops import bijectors\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/distributions/__init__.py\", line 44, in <module>\n","    from tensorflow.contrib.distributions.python.ops.estimator import *\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/distributions/python/ops/estimator.py\", line 21, in <module>\n","    from tensorflow.contrib.learn.python.learn.estimators.head import _compute_weighted_loss\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/__init__.py\", line 93, in <module>\n","    from tensorflow.contrib.learn.python.learn import *\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/__init__.py\", line 28, in <module>\n","    from tensorflow.contrib.learn.python.learn import *\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/__init__.py\", line 30, in <module>\n","    from tensorflow.contrib.learn.python.learn import estimators\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/__init__.py\", line 302, in <module>\n","    from tensorflow.contrib.learn.python.learn.estimators.dnn import DNNClassifier\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/dnn.py\", line 34, in <module>\n","    from tensorflow.contrib.learn.python.learn.estimators import dnn_linear_combined\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/dnn_linear_combined.py\", line 36, in <module>\n","    from tensorflow.contrib.learn.python.learn.estimators import estimator\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/estimators/estimator.py\", line 52, in <module>\n","    from tensorflow.contrib.learn.python.learn.learn_io import data_feeder\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/__init__.py\", line 26, in <module>\n","    from tensorflow.contrib.learn.python.learn.learn_io.dask_io import extract_dask_data\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/learn/python/learn/learn_io/dask_io.py\", line 33, in <module>\n","    import dask.dataframe as dd\n","  File \"/usr/local/lib/python3.7/dist-packages/dask/dataframe/__init__.py\", line 3, in <module>\n","    from . import backends, dispatch, rolling\n","  File \"/usr/local/lib/python3.7/dist-packages/dask/dataframe/backends.py\", line 18, in <module>\n","    from dask.array.dispatch import percentile_lookup\n","  File \"/usr/local/lib/python3.7/dist-packages/dask/array/__init__.py\", line 3, in <module>\n","    from . import backends, fft, lib, linalg, ma, overlap, random\n","  File \"/usr/local/lib/python3.7/dist-packages/dask/array/backends.py\", line 13, in <module>\n","    from .percentile import _percentile\n","  File \"/usr/local/lib/python3.7/dist-packages/dask/array/percentile.py\", line 11, in <module>\n","    from .core import Array\n","  File \"/usr/local/lib/python3.7/dist-packages/dask/array/core.py\", line 31, in <module>\n","    from fsspec import get_mapper\n","  File \"/usr/local/lib/python3.7/dist-packages/fsspec/__init__.py\", line 65, in <module>\n","    process_entries()\n","  File \"/usr/local/lib/python3.7/dist-packages/fsspec/__init__.py\", line 50, in process_entries\n","    eps = entry_points()\n","  File \"/usr/local/lib/python3.7/dist-packages/importlib_metadata/__init__.py\", line 1052, in entry_points\n","    return SelectableGroups.load(eps).select(**params)\n","  File \"/usr/local/lib/python3.7/dist-packages/importlib_metadata/__init__.py\", line 482, in load\n","    ordered = sorted(eps, key=by_group)\n","  File \"/usr/local/lib/python3.7/dist-packages/importlib_metadata/__init__.py\", line 1050, in <genexpr>\n","    dist.entry_points for dist in _unique(distributions())\n","  File \"/usr/local/lib/python3.7/dist-packages/importlib_metadata/__init__.py\", line 642, in entry_points\n","    return EntryPoints._from_text_for(self.read_text('entry_points.txt'), self)\n","  File \"/usr/local/lib/python3.7/dist-packages/importlib_metadata/__init__.py\", line 950, in read_text\n","    return self._path.joinpath(filename).read_text(encoding='utf-8')\n","  File \"/usr/lib/python3.7/pathlib.py\", line 922, in joinpath\n","    return self._make_child(args)\n","  File \"/usr/lib/python3.7/pathlib.py\", line 706, in _make_child\n","    self._drv, self._root, self._parts, drv, root, parts)\n","  File \"/usr/lib/python3.7/pathlib.py\", line 101, in join_parsed_parts\n","    def join_parsed_parts(self, drv, root, parts, drv2, root2, parts2):\n","KeyboardInterrupt\n"]}],"source":["!python3 DeepSpeech.py --early_stop True --epochs 10 -scorer deepspeech-0.9.3-models.scorer --export_dir model/Trained_Models \\\n","    --n_hidden 2048 \\\n","    --learning_rate 0.0001 \\\n","    --alphabet_config_path data/alphabet.txt \\\n","    --load_checkpoint_dir /content/mydrive/MyDrive/DeepSpeechv/deepspeech-0.9.3-checkpoint \\\n","    --save_checkpoint_dir /content/mydrive/MyDrive/DeepSpeechv/deepspeech-0.9.3-checkpoint \\\n","    --train_files data/Nemorous/train.csv   \\\n","    --dev_files   data/Nemorous/dev.csv \\\n","    --test_files  data/Nemorous/test.csv \\\n","    --summary_dir logs/log \\\n","    --alsologtostderr true \\\n","    --reduce_lr_on_plateau true \\\n","    --dropout_rate 0.3"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3479749,"status":"ok","timestamp":1668994114851,"user":{"displayName":"Chalmuri Mayuri Naidu","userId":"10695233935588128195"},"user_tz":480},"id":"Sv-sG2uJsC35","outputId":"1d62c2c7-d2ad-4ac0-a78b-c56bc1e59709"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/scipy/__init__.py:149: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.16.0\n","  UserWarning)\n","I Loading best validating checkpoint from /content/mydrive/MyDrive/DeepSpeechv/deepspeech-0.9.3-checkpoint/best_dev-1470251\n","I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n","I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n","I Loading variable from checkpoint: global_step\n","I Loading variable from checkpoint: layer_1/bias\n","I Loading variable from checkpoint: layer_1/weights\n","I Loading variable from checkpoint: layer_2/bias\n","I Loading variable from checkpoint: layer_2/weights\n","I Loading variable from checkpoint: layer_3/bias\n","I Loading variable from checkpoint: layer_3/weights\n","I Loading variable from checkpoint: layer_5/bias\n","I Loading variable from checkpoint: layer_5/weights\n","I Loading variable from checkpoint: layer_6/bias\n","I Loading variable from checkpoint: layer_6/weights\n","Testing model on data/Nemorous/train.csv\n","Test epoch | Steps: 298 | Elapsed Time: 0:57:46                                 \n","Test on data/Nemorous/train.csv - WER: 0.822707, CER: 0.593359, loss: 48.043350\n","--------------------------------------------------------------------------------\n","Best WER: \n","--------------------------------------------------------------------------------\n","WER: 0.666667, CER: 0.576923, loss: 56.786476\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/LL9.WAV\n"," - src: \"the goo is mowing the knew\"\n"," - res: \"the  i is en\"\n","--------------------------------------------------------------------------------\n","WER: 0.666667, CER: 0.600000, loss: 55.913174\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/MH30.WAV\n"," - src: \"the dive is searching the fife\"\n"," - res: \"the i is i e\"\n","--------------------------------------------------------------------------------\n","WER: 0.666667, CER: 0.655172, loss: 53.778732\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/MH2.WAV\n"," - src: \"the fate is serving the phase\"\n"," - res: \"the is i ie\"\n","--------------------------------------------------------------------------------\n","WER: 0.666667, CER: 0.571429, loss: 53.269249\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/FB72.WAV\n"," - src: \"the jew is weighing the face\"\n"," - res: \"the i is sii at\"\n","--------------------------------------------------------------------------------\n","WER: 0.666667, CER: 0.566667, loss: 51.477203\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/LL26.WAV\n"," - src: \"the bash is searching the bass\"\n"," - res: \"the  is sin t\"\n","--------------------------------------------------------------------------------\n","Median WER: \n","--------------------------------------------------------------------------------\n","WER: 0.833333, CER: 0.600000, loss: 48.211033\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/JF71.WAV\n"," - src: \"the pat is waving the zoo\"\n"," - res: \"the i isi  n\"\n","--------------------------------------------------------------------------------\n","WER: 0.833333, CER: 0.692308, loss: 48.144268\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/KS57.WAV\n"," - src: \"the bag is chewing the fin\"\n"," - res: \"the i iie\"\n","--------------------------------------------------------------------------------\n","WER: 0.833333, CER: 0.555556, loss: 48.098778\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/MH29.WAV\n"," - src: \"the thin is sitting the fay\"\n"," - res: \"the i i i  ae\"\n","--------------------------------------------------------------------------------\n","WER: 0.833333, CER: 0.538462, loss: 48.018314\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/LL5.WAV\n"," - src: \"the lot is surging the inn\"\n"," - res: \"the e i sig n\"\n","--------------------------------------------------------------------------------\n","WER: 0.833333, CER: 0.535714, loss: 47.980999\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/MH3.WAV\n"," - src: \"the bathe is shooing the rot\"\n"," - res: \"the a i sin t\"\n","--------------------------------------------------------------------------------\n","Worst WER: \n","--------------------------------------------------------------------------------\n","WER: 0.833333, CER: 0.520000, loss: 37.671600\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/FB33.WAV\n"," - src: \"the pat is wading the coo\"\n"," - res: \"the a i s in o\"\n","--------------------------------------------------------------------------------\n","WER: 0.833333, CER: 0.576923, loss: 37.267143\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/LL11.WAV\n"," - src: \"the bet is wearing the bit\"\n"," - res: \"the  i s sig at\"\n","--------------------------------------------------------------------------------\n","WER: 0.833333, CER: 0.423077, loss: 37.106697\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/JF19.WAV\n"," - src: \"the din is sinning the moo\"\n"," - res: \"the  i s sinn  o\"\n","--------------------------------------------------------------------------------\n","WER: 0.833333, CER: 0.461538, loss: 36.001606\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/JF55.WAV\n"," - src: \"the bag is sipping the fin\"\n"," - res: \"the  i s sin ttin\"\n","--------------------------------------------------------------------------------\n","WER: 1.000000, CER: 0.680000, loss: 55.050873\n"," - wav: file:///content/mydrive/MyDrive/DeepSpeechv/DeepSpeech/data/Nemorous/Wav/FB48.WAV\n"," - src: \"the cop is living the lot\"\n"," - res: \"te   iet\"\n","--------------------------------------------------------------------------------\n"]}],"source":["!python3 DeepSpeech.py \\\n","    --test_files data/Nemorous/test.csv \\\n","    --checkpoint_dir /content/mydrive/MyDrive/DeepSpeechv/deepspeech-0.9.3-checkpoint\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0GjJ-5gjuqW"},"outputs":[],"source":["!python3 DeepSpeech.py --early_stop True --epochs 1 -scorer deepspeech-0.9.3-models.scorer --export_dir model/Trained_Models \\\n","    --n_hidden 2048 \\\n","    --learning_rate 0.0001 \\\n","    --alphabet_config_path data/alphabet.txt \\\n","    --load_checkpoint_dir /content/mydrive/MyDrive/DeepSpeechv/deepspeech-0.9.3-checkpoint \\\n","    --save_checkpoint_dir /content/mydrive/MyDrive/DeepSpeechv/deepspeech-0.9.3-checkpoint \\\n","    --train_files data/TORGO/train.csv   \\\n","    --dev_files   data/TORGO/dev.csv \\\n","    --test_files  data/TORGO/test.csv \\\n","    --summary_dir logs/log \\\n","    --alsologtostderr true \\\n","    --reduce_lr_on_plateau true \\\n","    --dropout_rate 0.3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aImd8wAbj768"},"outputs":[],"source":["!python3 DeepSpeech.py \\\n","    --test_files data/TORGO/test.csv \\\n","    --checkpoint_dir /content/mydrive/MyDrive/DeepSpeechv/deepspeech-0.9.3-checkpoint"]}],"metadata":{"accelerator":"TPU","colab":{"authorship_tag":"ABX9TyM9dLe5/HzL2D4GwY8z3o7W","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
